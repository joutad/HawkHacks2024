{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35b3d71-2606-445a-8f86-dfd67b7fad31",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/joudat/anaconda3/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: numpy in /Users/joudat/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in /Users/joudat/anaconda3/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pillow in /Users/joudat/anaconda3/lib/python3.11/site-packages (9.4.0)\n",
      "Requirement already satisfied: keras in /Users/joudat/anaconda3/lib/python3.11/site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (2.0.0)\n",
      "Requirement already satisfied: numpy in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (1.24.3)\n",
      "Requirement already satisfied: rich in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /Users/joudat/anaconda3/lib/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from optree->keras) (4.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/joudat/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a190aff-6ddb-4226-a969-cb3af628782a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Directory paths\n",
    "input_dir = 'input_images'\n",
    "target_dir = 'target_images'\n",
    "intermediate_dir = 'intermediate_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b129b8a3-ea8f-4897-80f4-5654708b90e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 4\n",
      "Number of validation samples: 1\n"
     ]
    }
   ],
   "source": [
    "# Function to load images from a directory\n",
    "def load_images(image_dir):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(image_dir)):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            img = load_img(img_path, target_size=(512, 512))\n",
    "            img = img_to_array(img)\n",
    "            img = (img / 127.5) - 1  # Normalize to [-1, 1]\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load input and target images\n",
    "input_images = load_images(input_dir)\n",
    "target_images = load_images(target_dir)\n",
    "\n",
    "# Load intermediate images\n",
    "intermediate_images = []\n",
    "for subdir in sorted(os.listdir(intermediate_dir)):\n",
    "    intermediate_subdir = os.path.join(intermediate_dir, subdir)\n",
    "    if os.path.isdir(intermediate_subdir):\n",
    "        images = load_images(intermediate_subdir)\n",
    "        intermediate_images.append(images)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "input_dataset = tf.data.Dataset.from_tensor_slices(input_images)\n",
    "target_dataset = tf.data.Dataset.from_tensor_slices(target_images)\n",
    "\n",
    "# Convert intermediate images list to tuple of datasets\n",
    "intermediate_datasets = tuple(tf.data.Dataset.from_tensor_slices(steps) for steps in intermediate_images)\n",
    "\n",
    "# Combine into one dataset\n",
    "dataset = tf.data.Dataset.zip((input_dataset, intermediate_datasets, target_dataset))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "\n",
    "# Split into training and validation sets\n",
    "total_samples = len(input_images) + sum(len(steps) for steps in intermediate_images)\n",
    "train_size = int(0.8 * total_samples)\n",
    "\n",
    "# Split into training and validation sets\n",
    "val_size = total_samples - train_size\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "\n",
    "# Print dataset information\n",
    "# Print dataset information\n",
    "print(\"Number of training samples:\", train_size)\n",
    "print(\"Number of validation samples:\", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "470dddaf-24b6-4940-b328-998ca5c06bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input images: 1\n",
      "Number of intermediate images folders: 1\n",
      "Number of target images: 1\n",
      "Number of images in intermediate folder 1: 4\n"
     ]
    }
   ],
   "source": [
    "# Print lengths of input, intermediate, and target images\n",
    "print(\"Number of input images:\", len(input_images))\n",
    "print(\"Number of intermediate images folders:\", len(intermediate_images))\n",
    "print(\"Number of target images:\", len(target_images))\n",
    "\n",
    "# Print number of images in each intermediate folder\n",
    "for i, steps in enumerate(intermediate_images):\n",
    "    print(f\"Number of images in intermediate folder {i+1}:\", len(steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "46dcef24-9ec0-4670-b07b-871f0a55698c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joudat/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_47\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_47\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │ conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │ conv2d_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_36 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │ conv2d_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_27      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ conv2d_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_37 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,704</span> │ concatenate_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_28      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_38 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,208</span> │ concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_29      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_39 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,147</span> │ concatenate_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_80 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │      \u001b[38;5;34m3,136\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_81 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │    \u001b[38;5;34m131,200\u001b[0m │ conv2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_82 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m524,544\u001b[0m │ conv2d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_83 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │  \u001b[38;5;34m2,097,664\u001b[0m │ conv2d_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_36 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │  \u001b[38;5;34m2,097,408\u001b[0m │ conv2d_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_27      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m512\u001b[0m)              │            │ conv2d_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_37 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m1,048,704\u001b[0m │ concatenate_27[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_28      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_38 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │    \u001b[38;5;34m262,208\u001b[0m │ concatenate_28[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_29      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_39 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │      \u001b[38;5;34m6,147\u001b[0m │ concatenate_29[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,171,011</span> (23.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,171,011\u001b[0m (23.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,171,011</span> (23.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,171,011\u001b[0m (23.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8323200</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,323,201</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_84 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m510\u001b[0m, \u001b[38;5;34m510\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8323200\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │     \u001b[38;5;34m8,323,201\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,324,097</span> (31.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,324,097\u001b[0m (31.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,324,097</span> (31.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,324,097\u001b[0m (31.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Function to build the generator (U-Net architecture)\n",
    "def build_generator(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs)\n",
    "    conv2 = Conv2D(128, 4, strides=2, padding='same', activation='relu')(conv1)\n",
    "    conv3 = Conv2D(256, 4, strides=2, padding='same', activation='relu')(conv2)\n",
    "    conv4 = Conv2D(512, 4, strides=2, padding='same', activation='relu')(conv3)\n",
    "    \n",
    "    # Decoder\n",
    "    deconv1 = Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu')(conv4)\n",
    "    deconv1_concat = Concatenate()([deconv1, conv3])\n",
    "    deconv2 = Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu')(deconv1_concat)\n",
    "    deconv2_concat = Concatenate()([deconv2, conv2])\n",
    "    deconv3 = Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu')(deconv2_concat)\n",
    "    deconv3_concat = Concatenate()([deconv3, conv1])\n",
    "    outputs = Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')(deconv3_concat)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Function to build the discriminator (PatchGAN architecture)\n",
    "def build_discriminator(input_shape):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     conv1 = Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs)\n",
    "#     conv2 = Conv2D(128, 4, strides=2, padding='same', activation='relu')(conv1)\n",
    "#     conv3 = Conv2D(256, 4, strides=2, padding='same', activation='relu')(conv2)\n",
    "#     conv4 = Conv2D(512, 4, strides=2, padding='same', activation='relu')(conv3)\n",
    "#     outputs = Conv2D(1, 4, strides=1, padding='same', activation='sigmoid')(conv4)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output shape (batch_size, 1)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    # return Model(inputs, outputs)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (512, 512, 3)\n",
    "\n",
    "# Build generator and discriminator\n",
    "generator = build_generator(input_shape)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Display model summaries\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "69a33bfd-dd76-430f-bab1-e5f4a3ad2560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define optimizer for both generator and discriminator\n",
    "optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "# Compile generator\n",
    "generator.compile(optimizer=optimizer, loss='binary_crossentropy', run_eagerly=True)\n",
    "\n",
    "# Define GAN model\n",
    "discriminator.trainable = False  # Set discriminator to non-trainable\n",
    "gan_input = Input(shape=input_shape)\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "# Compile GAN\n",
    "gan.compile(optimizer=optimizer, loss='binary_crossentropy', run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3ea1f9dd-b8d8-4f19-a45e-b8839e7201aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "# Define function to generate batches of real images\n",
    "def generate_real_samples(dataset, batch_size):\n",
    "    # Shuffle and batch the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "    # Get a batch of images and labels\n",
    "    iterator = iter(dataset)\n",
    "    real_images_tuple = next(iterator)\n",
    "\n",
    "    # Print to understand the structure\n",
    "    print(\"Type of real_images_tuple:\", type(real_images_tuple))\n",
    "    \n",
    "    # Check if real_images_tuple contains the actual tensors\n",
    "    if isinstance(real_images_tuple, tuple):\n",
    "        for idx, item in enumerate(real_images_tuple):\n",
    "            print(f\"Index {idx} in real_images_tuple:\", item)\n",
    "    \n",
    "    # Assume the actual images are in the first element of the tuple\n",
    "    real_images = real_images_tuple[0]\n",
    "\n",
    "    print(\"Shape of real_images before reshaping:\", real_images.shape)\n",
    "\n",
    "    # If the shape is (1, 3, 512, 512, 3), reshape to (batch_size, 512, 512, 3)\n",
    "    if real_images.shape == (1, 1, 512, 512, 3):\n",
    "        real_images = tf.reshape(real_images, (batch_size, 512, 512, 3))\n",
    "    else:\n",
    "        # Print and debug if shape is different\n",
    "        print(\"Unexpected shape:\", real_images.shape)\n",
    "    \n",
    "    print(\"Shape of real_images after reshaping:\", real_images.shape)\n",
    "\n",
    "    labels = tf.ones((batch_size, 1))  # Label real images as 1 (real)\n",
    "    return real_images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ad3bad04-334c-4831-9b09-202967a0b007",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assuming dataset is defined elsewhere and properly structured\n",
    "# dataset = ...\n",
    "batch_size = 1\n",
    "real_images, labels = generate_real_samples(dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c96bf67e-afc9-4c7d-b662-f1324930d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to train the discriminator\n",
    "\n",
    "def train_discriminator(discriminator, generator, dataset, batch_size):\n",
    "    # Generate real samples\n",
    "    real_images, real_labels = generate_real_samples(dataset, batch_size)\n",
    "\n",
    "    # Generate fake samples\n",
    "    noise = np.random.randn(batch_size, 512, 512, 3)\n",
    "    fake_images = generator.predict(noise)\n",
    "    fake_labels = np.zeros((batch_size, 1))  # Label fake images as 0 (fake)\n",
    "\n",
    "    # Train discriminator on real and fake samples\n",
    "    d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    print(f'Discriminator loss: {d_loss}')\n",
    "    return d_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "40bcebd3-b8c9-4cb3-a3d0-bcb9106197bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to train the generator\n",
    "def train_generator(gan_model, batch_size):\n",
    "    noise = np.random.randn(batch_size, 512, 512, 3)\n",
    "    labels = np.ones((batch_size, 1))  # Label generated images as 1 (real)\n",
    "    g_loss = gan_model.train_on_batch(noise, labels)\n",
    "    \n",
    "    print(f'Generator loss: {g_loss}')\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "80a60c79-e35c-4e45-8308-18c895ed11d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_generated_images(generator, epoch, batch, noise_dim=(512, 512, 3), examples=3):\n",
    "    noise = np.random.randn(examples, *noise_dim)\n",
    "    generated_images = generator.predict(noise)\n",
    "    \n",
    "    for i, img in enumerate(generated_images):\n",
    "        plt.subplot(1, examples, i+1)\n",
    "        plt.imshow((img * 127.5 + 127.5).astype(np.uint8))  # Assuming generator output is in range [-1, 1]\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.savefig(f'generated_images_epoch{epoch}_batch{batch}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6ff80f65-1232-4cd6-a949-453428f91c31",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "Discriminator loss: [0.79300851 0.29670331]\n",
      "Generator loss: [array(0.79212034), array(0.79212034), array(0.79212034), array(0.2857143)]\n",
      "Epoch 1/10 - Batch 1/1 - Discriminator Loss: [0.79300851 0.29670331], Generator Loss: [array(0.79212034), array(0.79212034), array(0.79212034), array(0.2857143)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "Discriminator loss: [0.79855916 0.25833334]\n",
      "Generator loss: [array(0.79857951), array(0.79857951), array(0.79857951), array(0.25)]\n",
      "Epoch 2/10 - Batch 1/1 - Discriminator Loss: [0.79855916 0.25833334], Generator Loss: [array(0.79857951), array(0.79857951), array(0.79857951), array(0.25)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "Discriminator loss: [0.81477544 0.22875817]\n",
      "Generator loss: [array(0.82568794), array(0.82568794), array(0.82568794), array(0.22222222)]\n",
      "Epoch 3/10 - Batch 1/1 - Discriminator Loss: [0.81477544 0.22875817], Generator Loss: [array(0.82568794), array(0.82568794), array(0.82568794), array(0.22222222)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "Discriminator loss: [0.8571986  0.20526316]\n",
      "Generator loss: [array(0.88540876), array(0.88540876), array(0.88540876), array(0.2)]\n",
      "Epoch 4/10 - Batch 1/1 - Discriminator Loss: [0.8571986  0.20526316], Generator Loss: [array(0.88540876), array(0.88540876), array(0.88540876), array(0.2)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step\n",
      "Discriminator loss: [0.93572444 0.18614719]\n",
      "Generator loss: [array(0.98589778), array(0.98589778), array(0.98589778), array(0.18181819)]\n",
      "Epoch 5/10 - Batch 1/1 - Discriminator Loss: [0.93572444 0.18614719], Generator Loss: [array(0.98589778), array(0.98589778), array(0.98589778), array(0.18181819)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "Discriminator loss: [1.04184783 0.17028986]\n",
      "Generator loss: [array(1.10203707), array(1.10203707), array(1.10203707), array(0.16666667)]\n",
      "Epoch 6/10 - Batch 1/1 - Discriminator Loss: [1.04184783 0.17028986], Generator Loss: [array(1.10203707), array(1.10203707), array(1.10203707), array(0.16666667)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "Discriminator loss: [1.15634698 0.15692308]\n",
      "Generator loss: [array(1.2192024), array(1.2192024), array(1.2192024), array(0.15384616)]\n",
      "Epoch 7/10 - Batch 1/1 - Discriminator Loss: [1.15634698 0.15692308], Generator Loss: [array(1.2192024), array(1.2192024), array(1.2192024), array(0.15384616)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step\n",
      "Discriminator loss: [1.27010339 0.14550265]\n",
      "Generator loss: [array(1.33325636), array(1.33325636), array(1.33325636), array(0.14285715)]\n",
      "Epoch 8/10 - Batch 1/1 - Discriminator Loss: [1.27010339 0.14550265], Generator Loss: [array(1.33325636), array(1.33325636), array(1.33325636), array(0.14285715)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step\n",
      "Discriminator loss: [1.37934577 0.13563219]\n",
      "Generator loss: [array(1.44077516), array(1.44077516), array(1.44077516), array(0.13333334)]\n",
      "Epoch 9/10 - Batch 1/1 - Discriminator Loss: [1.37934577 0.13563219], Generator Loss: [array(1.44077516), array(1.44077516), array(1.44077516), array(0.13333334)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827ms/step\n",
      "Type of real_images_tuple: <class 'tuple'>\n",
      "Index 0 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Index 1 in real_images_tuple: (<tf.Tensor: shape=(1, 1, 512, 512, 3), dtype=float32, numpy=\n",
      "array([[[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]]], dtype=float32)>,)\n",
      "Index 2 in real_images_tuple: tf.Tensor(\n",
      "[[[[[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]\n",
      "\n",
      "   [[1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    ...\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]\n",
      "    [1. 1. 1.]]]]], shape=(1, 1, 512, 512, 3), dtype=float32)\n",
      "Shape of real_images before reshaping: (1, 1, 512, 512, 3)\n",
      "Shape of real_images after reshaping: (1, 512, 512, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step\n",
      "Discriminator loss: [1.48229259 0.12701613]\n",
      "Generator loss: [array(1.5416286), array(1.5416286), array(1.5416286), array(0.125)]\n",
      "Epoch 10/10 - Batch 1/1 - Discriminator Loss: [1.48229259 0.12701613], Generator Loss: [array(1.5416286), array(1.5416286), array(1.5416286), array(0.125)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    # print(len(dataset))\n",
    "    # print(batch_size)\n",
    "    # print(len(dataset) // batch_size)\n",
    "    for batch in range(len(dataset) // batch_size):\n",
    "        # Train discriminator\n",
    "        d_loss = train_discriminator(discriminator, generator, dataset, batch_size)\n",
    "\n",
    "        # Train generator (via GAN model)\n",
    "        g_loss = train_generator(gan, batch_size)\n",
    "\n",
    "        # Print loss after each batch\n",
    "        # print(f'Epoch {epoch + 1}/{epochs} - Batch {batch + 1}/{len(dataset) // batch_size} - Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}')\n",
    "        # print(f'Epoch {epoch + 1}/{epochs} - Batch {batch + 1}/{len(input_dataset) // batch_size} - Discriminator Loss: {d_loss}, Generator Loss: {g_loss}')\n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Batch {batch + 1}/{len(input_dataset) // batch_size} - Discriminator Loss: {d_loss}, Generator Loss: {g_loss}')\n",
    "        \n",
    "        # Save generated images\n",
    "        save_generated_images(generator, epoch + 1, batch + 1)\n",
    "        \n",
    "    # Optionally, monitor training progress after each epoch\n",
    "    # print(f'Epoch {epoch + 1}/{epochs} - Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693d725-b05e-4709-9c6d-e17df02bb797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
